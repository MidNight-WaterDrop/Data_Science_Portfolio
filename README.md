# Data Scientist & Business Intelligence Analyst Portfolio

#### Technical Skills:
- **Python**: Highly proficient in data manipulation and analysis using libraries like **Pandas** (e.g., merging datasets, handling missing data), **NumPy** (e.g., array operations, linear algebra), and machine learning with **Scikit-learn** (e.g., classification, regression) and **XGBoost** (e.g., gradient boosting). Experienced in deep learning frameworks such as **TensorFlow** and **Keras** for neural network development.  
- **SQL**: Advanced skills in writing complex queries for data extraction, aggregation, and transformation using **MySQL**, **PostgreSQL**, and **SQL Server**. Proficient in optimizing database performance and designing schemas.  
- **AWS**: Skilled in cloud solutions including S3 (storage), EC2 (compute), Lambda (serverless), Redshift (data warehousing), and **ECS (Elastic Container Service)** for orchestrating Docker containers, enabling scalable and efficient data pipelines.
- **Tableau**: Expert in designing interactive, user-friendly dashboards for stakeholders, integrating multiple data sources, and creating visualizations (e.g., heatmaps, trend lines) for real-time KPI monitoring.  
- **R**: Proficient in statistical modeling (e.g., hypothesis testing, regression analysis) and data visualization using packages like **ggplot2** and **dplyr** for data wrangling.
---

## Education
- **M.S., Data Analytics | The Pennsylvania State University (_December 2024_)**  
  - **GPA**: 3.87 / 4.0  
  - **Relevant Coursework**:  
    - *Deep Learning*: Built neural networks for image classification using TensorFlow.  
    - *Predictive Analytics*: Applied time-series forecasting models to real-world datasets.  
    - *Analytics Programming in Python*: Developed end-to-end data pipelines.  
    - *Data Mining*: Explored clustering and association rule mining techniques.  
    - *Large-Scale Database and Warehouse*: Designed scalable data storage solutions.  
  - **Capstone Project**: Created a customer churn prediction model using logistic regression and random forests in Python, leveraging SQL for data preprocessing. Achieved an accuracy of 85% and presented findings to a panel of industry experts.  

- **B.A., International Trade and Business | Tung-Hai University (_June 2029_)**  
  - **GPA**: 3.4 / 4.0, Graduated with Honors  
  - **Core Subjects**:  
    - *Project Management*: Led a team to analyze supply chain inefficiencies.  
    - *Statistics*: Conducted hypothesis testing and ANOVA on trade data.  
    - *Data Visualization*: Created charts and graphs to communicate trends.  
  - **Thesis**: "Global Trade Patterns: A Statistical Analysis" – Used R to analyze 10 years of trade data, identifying key economic drivers. Presented at Tung-Hai University’s Annual Research Symposium, earning a top presentation award.  

---

## Work Experience
**Business Intelligence Analyst @ TBL Services Inc. (_January 2025 - Present_)**  
- Designed and optimized **SQL Server** queries to enhance freight payment analysis, reducing processing time by 30% and improving data accuracy for financial reporting.  
- Built **Python-based ETL pipelines** to ingest and transform data from disparate sources (e.g., CSV files, APIs), enabling account managers to access clean datasets 20% faster.  
- Deployed automated data workflows in **AWS**, using **S3** for storage, **Lambda** for event-driven processing, and **ECS** to orchestrate **Docker containers**, enhancing scalability and cutting deployment times by 20%.
- Created automated reporting tools with **Python** (e.g., using Matplotlib for plots) and **Tableau** dashboards, delivering weekly insights to business data scientists with 100% uptime.  

**Logistics Analyst @ TBL Services Inc. (_March 2023 - December 2026_)**  
- Automated logistics workflows with **Python ETL scripts**, integrating data from warehouse systems and reducing manual processing time by 25%.  
- Developed tailored **SQL datasets** for customer-specific reporting, improving client satisfaction scores by 15% through precise, actionable insights.  
- Implemented **Recurrent Neural Networks (RNNs)** in Python to cleanse noisy logistics data, boosting reporting accuracy by 15% and enabling better forecasting.  
- Collaborated with supply chain teams to introduce data-driven process improvements, cutting operational costs by 10% through optimized resource allocation.  

**Logistics Coordinator @ TBL Services Inc. (_February 2021 - February 2023_)**  
- Utilized **Tableau** to analyze logistics costs, identifying inefficiencies that led to an 8% annual cost reduction across transportation channels.  
- Designed dynamic **Tableau dashboards** with real-time KPIs (e.g., shipping delays, fuel costs), accelerating decision-making by 20% for logistics managers.  
- Maintained data integrity in shipping documentation using **ERP systems**, reducing errors by 15% and ensuring compliance with regulatory standards.  
- Executed cost-saving strategies by analyzing historical data, streamlining workflows, and negotiating vendor contracts based on insights.  

---

## Projects
### Water Potability Prediction System  
- **Objective**: Built a predictive model to classify water as potable or non-potable using physicochemical properties (e.g., pH, hardness, turbidity).  
- **Dataset**: Utilized a Kaggle dataset with 3,276 samples and 9 features, addressing missing values through imputation.  
- **Methodology**: Trained **Random Forest** and **XGBoost** models, tuning hyperparameters with **Bayesian Optimization** to achieve a 12% accuracy boost (final accuracy: 89%).  
- **Insights**: Identified pH and sulfate levels as top predictors, informing water treatment recommendations.  
- **Technologies**: Python (Scikit-learn, XGBoost), Pandas, NumPy, Matplotlib for visualization.  

### E-commerce User Behavior Analysis with Deep Learning  
- **Objective**: Predicted user purchase intent to enhance e-commerce personalization.  
- **Dataset**: Processed 5.4 million user interaction records (clicks, views, purchases) from an e-commerce platform.  
- **Methodology**: Implemented a **DeepFM** model combining factorization machines and deep neural networks. Preprocessed data with one-hot encoding and embeddings, achieving a **ROC-AUC of 0.9998**.  
- **Impact**: Improved recommendation accuracy by 18%, driving potential revenue growth for large-scale platforms.  
- **Technologies**: Python (TensorFlow, Keras), SQL for data extraction, AWS (S3, EC2) for deployment.  

### Big Data Processing & Cloud Computing  
- **Objective**: Optimized internal processes through scalable data analysis.  
- **Methodology**: Deployed **Hadoop** on a 5-node cluster to process 10TB of operational data. Designed ETL workflows in **KNIME** to clean and aggregate data, applying **association rule mining** (e.g., Apriori algorithm) to uncover process bottlenecks.  
- **Impact**: Recommended workflow changes that improved efficiency by 10%, validated through A/B testing.  
- **Technologies**: Hadoop, KNIME, AWS (S3 for storage, EC2 for computation), Python for scripting.  

---

## Additional Information
- **Courses**:  
  - *Java Programming and Software Engineering Fundamentals* (Coursera, Oct 2024 - Jan 2025): Mastered object-oriented programming and design patterns.  
  - *Algorithms* (Coursera, Oct 2024 - Jan 2025): Studied sorting, searching, and graph algorithms.  
  - *Computer Organization* (Coursera, Nov 2024 - Jan 2025): Explored CPU architecture and memory systems.  
  - *Operating Systems* (Coursera, Nov 2024 - Jan 2025): Learned process management and threading.  
  - *Mathematics for Computer Science* (Coursera, Dec 2024 - Jan 2025): Strengthened skills in discrete math and probability.   

- **Languages**:  
  - **Mandarin**: Native Speaker.

- **Certifications**:  
  - AWS Certified Cloud Practitioner (2025).
